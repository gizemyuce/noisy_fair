tensor([ 0.0352,  0.0032, -0.0008,  ...,  0.0010,  0.0034,  0.0008])
CMM train_err=0.0, err=44.53000259399414
1.138504147529602
1.0824514627456665
1.0317168235778809
0.9858911037445068
0.9445959329605103
0.9074749946594238
0.8742016553878784
0.8444619178771973
0.8179624080657959
0.794433057308197
0.7736125588417053
0.7552595138549805
0.7391432523727417
0.7250521183013916
0.7127818465232849
0.7021452784538269
0.6929661631584167
0.6850813627243042
0.6783389449119568
0.6726001501083374
0.6677383184432983
0.663638710975647
0.6601974964141846
0.6573216319084167
0.6549280285835266
0.6529450416564941
0.6513067483901978
0.6499607563018799
0.6488561034202576
0.6479557156562805
0.6472210884094238
0.6466249823570251
0.6461436748504639
0.6457533836364746
0.6454396843910217
0.6451867818832397
0.6449841260910034
0.6448218822479248
0.6446918249130249
0.6445883512496948
0.6445051431655884
0.6444389224052429
0.6443864107131958
0.6443445682525635
0.6443113088607788
0.6442849040031433
0.6442639827728271
0.6442468166351318
0.6442336440086365
0.6442227959632874
0.6442149877548218
0.6442087888717651
0.644203245639801
0.6441987752914429
0.644195556640625
0.6441929936408997
0.6441909074783325
0.6441894769668579
0.6441881060600281
0.6441874504089355
0.6441860795021057
0.6441856026649475
0.6441847085952759
0.6441845297813416
0.6441840529441833
0.6441836357116699
0.6441835761070251
0.6441835761070251
0.6441831588745117
0.6441830396652222
0.6441830992698669
0.6441829800605774
0.6441829204559326
0.6441828608512878
0.6441826820373535
0.6441828012466431
0.644182562828064
tensor([ 0.0819,  0.0068, -0.0017,  ...,  0.0013,  0.0078,  0.0020],
       requires_grad=True)
w=1.0, train_err=0.0, err=44.400001525878906
====================l1=========================
CMM train_err=0.0, err=6.610000133514404
9552.396484375
11869.728515625
5645.73193359375
2437.051025390625
19568.12890625
12456.453125
7467.61279296875
4200.12939453125
70364.890625
60829.3203125
52178.3203125
44389.26953125
37485.20703125
31416.255859375
26113.35546875
21545.833984375
17645.6015625
14299.38671875
11520.748046875
9197.794921875
7240.2060546875
7193.32666015625
4390.90576171875
6720.7177734375
3267.431640625
2007.6796875
1751.63720703125
1692.15283203125
76957648.0
76946872.0
76936344.0
76925600.0
76915128.0
76904448.0
76893800.0
76883128.0
76872600.0
76861992.0
76851352.0
76840592.0
76830144.0
76819440.0
76808880.0
76798096.0
76787704.0
76777080.0
76766432.0
76755720.0
76745328.0
76734664.0
76724032.0
76713344.0
76702912.0
76692320.0
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 431, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 294, in margin_classifiers_perf
    l.backward()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\__init__.py", line 166, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\__init__.py", line 68, in _make_grads
    new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))
KeyboardInterrupt