tensor([ 3.2898e-02,  3.5208e-03,  8.1409e-06,  ...,  2.2848e-03,
         3.8508e-03, -2.0257e-03])
CMM train_err=0.0, err=44.470001220703125
68654.78125
2184128.5
8476714.0
324557.40625
2983.847900390625
2405.115234375
104.12590789794922
6828809.0
964.5042114257812
841240.5
421.673583984375
1254.869140625
28427.537109375
9248.5341796875
8556982.0
248.85153198242188
57.38774490356445
7614.7880859375
11448706048.0
1341.9173583984375
4129.9072265625
1900.2406005859375
267132.375
1217.46142578125
43541.6484375
75143.78125
2291.112548828125
12224.078125
118117447499776.0
106.1854248046875
6458.0703125
2437.18017578125
414.28515625
5744.30419921875
2222.640380859375
13731.279296875
581.8475952148438
624.6094360351562
38286.9375
5390.04736328125
2435.9248046875
1323.0965576171875
662173504.0
2946.6484375
151.87596130371094
1825.4989013671875
22312.09765625
39337.3203125
11188.0146484375
645.11767578125
27667.810546875
184.5725555419922
49245.49609375
2340955.0
74.37955474853516
18.01536750793457
1439.7991943359375
2270834.25
110.43524169921875
25540.615234375
1399.6513671875
17106.568359375
9224.5712890625
26280.486328125
82424545280.0
141.11373901367188
529.4722900390625
707204.4375
718.5707397460938
876.4996948242188
199.1532440185547
721.2447509765625
114.94497680664062
6020.146484375
675.7794799804688
16.24140167236328
176164.859375
179.04429626464844
2603135.25
5341.494140625
3706.694091796875
8446.1005859375
93936.734375
102.48622131347656
7542.45068359375
965.5151977539062
1183.02294921875
5258.30712890625
2997.43798828125
373.5419616699219
213.8480682373047
1197385.5
8197.2646484375
11912.740234375
23370.455078125
1724071.625
22500.640625
26954.310546875
72936.921875
16873.41015625
520.3175048828125
26646.517578125
6677.78515625
931.22998046875
16334.060546875
1745.9019775390625
4994712.0
468.7897644042969
85.16549682617188
17738.740234375
13381.7216796875
145.87271118164062
27184.724609375
14695.4833984375
712.030517578125
123828.3671875
15532.904296875
852.1993408203125
365.26593017578125
40885.9609375
18217.9921875
170971.71875
8863.2587890625
2446.138671875
3435.1650390625
122.34602355957031
440833824.0
116.44496154785156
11627.9560546875
15370.541015625
64520.86328125
22082.62890625
67762.6796875
184.50148010253906
272551.03125
43241.91015625
591.2833862304688
5982.603515625
27139882.0
7563590.5
1602.9329833984375
121974704.0
1646.19580078125
644614.6875
28946.35546875
569105.6875
980.0355224609375
380.72515869140625
83362200.0
1019551.4375
91631.234375
513.9147338867188
5950.00634765625
659717.75
150.9462432861328
1733.0999755859375
14396.158203125
1006.1905517578125
25345842.0
3613333.25
6430.19287109375
4471.4990234375
5219.96630859375
735.3786010742188
31244.599609375
48040.92578125
88665.78125
36664.1015625
275641.40625
161.43710327148438
777.9500122070312
2251.422119140625
527487.625
5570.52978515625
50.47602462768555
2235.191650390625
2831.922607421875
1135.220458984375
228.9263458251953
79009408.0
11706.49609375
1130.7529296875
17356.298828125
240.0240936279297
10104.2421875
41916.0703125
221.61643981933594
2355.609619140625
4016565.0
295606.375
371888.71875
43827.78515625
303.83203125
7420.0966796875
935828.6875
341.4227294921875
420490.5
568.8123168945312
75018.671875
1428.7958984375
5319.45703125
176.04237365722656
668.3142700195312
2903.5615234375
726825.375
310.11199951171875
15993351168.0
556832.75
5299.693359375
401883.25
441302.25
29404.44921875
139710.25
379.87109375
35665.58203125
291733.03125
566.013427734375
6167.349609375
7345.95361328125
15361.6328125
3714.500244140625
496.5361328125
1638.1627197265625
367.74847412109375
1859.7008056640625
51776680.0
5338.3935546875
470304.21875
5624.06005859375
8700.8564453125
32378.66796875
4325074010112.0
128.85870361328125
1423091072.0
200.01251220703125
2150259.5
194.4923553466797
497430.5
5446.5888671875
1883.3472900390625
5976.02587890625
164280.703125
53160.4609375
1720.6212158203125
538.142822265625
133.46771240234375
418.53155517578125
1465789.0
93.61172485351562
536040.375
65630.265625
39857.6953125
5457.75341796875
103.93914794921875
770.8705444335938
1508.72314453125
7923.2666015625
210302.65625
7485.4736328125
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 426, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 223, in margin_classifiers_perf
    l = loss_average_poly(w, b, z1s, z2s, n1, n2) + torch.norm(w)**2
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 26, in loss_average_poly
    return (torch.sum(1./(z1s @ v)) + b * torch.sum(1./(z2s @ v))) /(n1+b*n2)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\_tensor.py", line 26, in wrapped
    @functools.wraps(f, assigned=assigned)
KeyboardInterrupt