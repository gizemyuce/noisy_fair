tensor([ 0.0320,  0.0036, -0.0003,  ...,  0.0017,  0.0042,  0.0028])
CMM train_err=0.0, err=44.380001068115234
1.1410752534866333
1.085183024406433
1.03459894657135
0.9889194965362549
0.9477629661560059
0.9107795357704163
0.8776339292526245
0.8480151891708374
0.8216323852539062
0.7982097268104553
0.7774876356124878
0.7592290043830872
0.7432000637054443
0.7291878461837769
0.716989278793335
0.7064186334609985
0.6972986459732056
0.6894677877426147
0.6827716827392578
0.6770763993263245
0.6722534894943237
0.6681873798370361
0.6647757291793823
0.6619256734848022
0.6595547199249268
0.6575920581817627
0.6559716463088989
0.6546408534049988
0.6535508632659912
0.6526610851287842
0.6519367694854736
0.6513497829437256
0.6508746147155762
0.6504912972450256
0.650183379650116
0.6499356627464294
0.6497362852096558
0.6495777368545532
0.6494500637054443
0.6493490934371948
0.6492679715156555
0.6492038369178772
0.6491522192955017
0.6491120457649231
0.6490800380706787
0.6490545272827148
0.6490339636802673
0.6490177512168884
0.6490048170089722
0.6489951014518738
0.6489869952201843
0.6489807367324829
0.6489756107330322
0.648971676826477
0.6489679217338562
0.648966372013092
0.648963987827301
0.6489624977111816
0.6489614248275757
0.6489604115486145
0.6489598155021667
0.648958683013916
0.6489583253860474
0.6489577889442444
0.6489576697349548
0.648957371711731
0.6489574909210205
0.6489568948745728
0.6489570140838623
0.6489568948745728
0.648956835269928
0.6489567756652832
0.6489568948745728
0.6489570140838623
0.648956835269928
0.6489568948745728
tensor([ 0.0736,  0.0080, -0.0010,  ...,  0.0050,  0.0088,  0.0063],
       requires_grad=True)
w=1.0, train_err=0.0, err=44.415000915527344
====================l1=========================
CMM train_err=0.0, err=6.154999732971191
305027.34375
295334.40625
285822.21875
276503.5
267384.75
258466.265625
249765.140625
241274.453125
232956.5625
224828.53125
217300.65625
209957.9375
202796.515625
195783.59375
188928.671875
182236.671875
175712.859375
169337.328125
163133.40625
157115.234375
151557.25
146143.609375
140862.140625
135716.703125
130714.390625
125865.8359375
121149.3125
116568.8671875
112126.8125
107822.9921875
103847.359375
99989.109375
96256.1875
92644.078125
89138.3203125
85735.875
82437.953125
79258.4921875
76172.9375
73196.4609375
70466.671875
67815.171875
65226.38671875
62724.72265625
60303.0546875
57948.63671875
55668.22265625
53465.84765625
51342.06640625
49288.890625
47395.10546875
45558.38671875
43772.30859375
42048.65234375
40376.484375
40057.88671875
514053.25
506376.1875
498771.96875
491246.03125
484162.125
477143.8125
470207.625
463366.875
456607.9375
449921.46875
443296.40625
436731.0
430237.9375
423817.125
417777.4375
411788.65625
405862.40625
399986.65625
394163.96875
388402.21875
382698.34375
377055.0
371476.25
365962.25
360765.84375
355619.75
350519.75
345471.28125
340463.625
335507.96875
330604.0
325755.875
320969.21875
316231.40625
311769.09375
307345.625
302967.875
298632.65625
294342.46875
290094.78125
285887.0
281712.84375
277577.6875
273481.53125
269628.0625
265809.53125
262032.296875
258297.421875
254607.484375
250961.15625
247356.703125
243782.359375
240233.953125
236720.25
233415.71875
230141.453125
226889.03125
223669.078125
220499.40625
217371.203125
214276.625
211218.234375
208194.75
205205.21875
202407.46875
199646.59375
196912.328125
194209.390625
191538.6875
188894.578125
186285.515625
183714.96875
181170.265625
178644.5
176272.5
173929.34375
171609.375
169313.15625
167039.515625
164791.109375
162567.515625
160369.6875
158196.0625
156043.75
154021.671875
152019.421875
150034.265625
148067.875
146125.859375
144209.8125
142315.5625
140443.1875
138588.25
136755.25
135033.796875
133330.65625
131642.09375
129970.2578125
128314.5
126675.5625
125054.3046875
123450.9765625
121863.2578125
120291.7421875
118816.5234375
117355.5625
115909.015625
114468.75
113038.8984375
111620.2109375
110213.5625
108813.0546875
107427.3984375
106059.2109375
104774.6328125
103502.53125
102242.1328125
100991.046875
99755.0859375
98531.359375
97321.296875
96125.78125
94938.5625
93764.1328125
92665.453125
91578.1171875
90500.4375
89435.8671875
88382.6015625
87342.5
86315.0
85294.3515625
84280.296875
83273.90625
82330.5703125
81402.578125
80483.1875
79570.2578125
78668.03125
77776.2421875
76896.0234375
76023.59375
75158.5234375
74302.515625
73495.171875
72693.234375
71901.8515625
71115.390625
70332.46875
69557.984375
68793.3515625
68038.7265625
67289.8203125
66549.546875
65852.21875
65160.60546875
64473.203125
63789.98046875
63112.91796875
62443.99609375
61779.6875
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 433, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 295, in margin_classifiers_perf
    l.backward()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt