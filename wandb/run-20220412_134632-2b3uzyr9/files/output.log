
tensor([ 0.0307,  0.0020,  0.0021,  ..., -0.0036,  0.0016, -0.0016])
CMM train_err=0.0, err=45.53499984741211
1.1386979818344116
1.0826458930969238
1.0319076776504517
0.9860758781433105
0.9447752237319946
0.9076460599899292
0.8743577003479004
0.8446017503738403
0.8180857300758362
0.7945348024368286
0.7736911177635193
0.7553131580352783
0.7391705513000488
0.725050687789917
0.7127518653869629
0.7020865678787231
0.692878246307373
0.6849638223648071
0.6781938076019287
0.6724295616149902
0.667542576789856
0.663420557975769
0.6599583029747009
0.657062828540802
0.6546521186828613
0.6526530981063843
0.6510021686553955
0.6496435403823853
0.6485298871994019
0.6476200222969055
0.646878719329834
0.6462767124176025
0.645789384841919
0.6453946828842163
0.6450774073600769
0.6448215842247009
0.6446169018745422
0.6444526314735413
0.6443203091621399
0.6442151069641113
0.6441313028335571
0.6440643072128296
0.6440104842185974
0.6439684629440308
0.6439344882965088
0.6439077854156494
0.6438862085342407
0.6438692212104797
0.6438555121421814
0.6438448429107666
0.6438363194465637
0.6438295245170593
0.6438239812850952
0.6438199281692505
0.6438167095184326
0.6438136100769043
0.6438118815422058
0.6438100337982178
0.6438085436820984
0.6438077688217163
0.6438068151473999
0.643805980682373
0.6438053250312805
0.643804669380188
0.6438045501708984
0.6438039541244507
0.6438040733337402
0.6438041925430298
0.6438039541244507
0.6438040137290955
0.6438037157058716
0.6438037157058716
0.6438036561012268
0.6438032388687134
0.6438031196594238
0.6438033580780029
C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\sklearn\svm\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
0.6438029408454895
0.6438030004501343
tensor([ 0.0713,  0.0039,  0.0048,  ..., -0.0070,  0.0037, -0.0036],
       requires_grad=True)
w=1.0, train_err=0.0, err=45.470001220703125
====================l1=========================
CMM train_err=0.0, err=6.840000152587891
3.457595109939575
3.4499590396881104
3.4494223594665527
3.4451797008514404
3.446809768676758
3.444103240966797
3.4462242126464844
3.4408187866210938
3.436164379119873
3.4409127235412598
3.4293406009674072
3.43056321144104
3.4317619800567627
3.4317052364349365
3.4315881729125977
3.4301939010620117
3.42899751663208
3.4296786785125732
3.4296627044677734
3.424454689025879
3.4180703163146973
3.4181101322174072
3.4200682640075684
3.4190845489501953
3.419888973236084
3.417672634124756
3.4217138290405273
3.419970989227295
3.4158782958984375
3.416870594024658
3.4060959815979004
3.40830659866333
3.4096713066101074
3.4084606170654297
3.405017375946045
3.4077868461608887
3.410372734069824
3.407824993133545
3.4071927070617676
3.4082860946655273
3.3959507942199707
3.397486686706543
3.3978800773620605
3.4017467498779297
3.3973116874694824
3.3987624645233154
3.4001426696777344
3.401618003845215
3.398994207382202
3.4006593227386475
3.389024257659912
3.387625217437744
3.3879051208496094
3.387509346008301
3.3862390518188477
3.3927907943725586
3.383272171020508
3.3908462524414062
3.3870959281921387
3.386064291000366
3.3763604164123535
3.3785500526428223
3.3816418647766113
3.3780064582824707
3.377289295196533
3.3776960372924805
3.3809518814086914
3.376612663269043
3.377098560333252
3.3828344345092773
3.3718247413635254
3.37172794342041
3.369314193725586
3.371023654937744
3.3702311515808105
3.3700485229492188
3.371906280517578
3.369438886642456
3.370999336242676
3.3698787689208984
3.361478090286255
3.362161874771118
3.362751007080078
3.3616833686828613
3.362034797668457
3.3621158599853516
3.3633387088775635
3.3611044883728027
3.362959384918213
3.362658977508545
3.3544750213623047
3.354384422302246
3.3561270236968994
3.3536882400512695
3.353611469268799
3.3548734188079834
3.3542680740356445
3.3555688858032227
3.354464054107666
3.3515446186065674
3.3443424701690674
3.3452281951904297
3.3460116386413574
3.347014904022217
3.346491575241089
3.3467836380004883
3.3457682132720947
3.347546100616455
3.348682165145874
3.3494668006896973
3.3385372161865234
3.338331937789917
3.341506004333496
3.3393893241882324
3.3382797241210938
3.3410212993621826
3.338244915008545
3.340221405029297
3.338447093963623
3.339522123336792
3.3318676948547363
3.3325209617614746
3.331599712371826
3.3337955474853516
3.332530975341797
3.3328471183776855
3.332148551940918
3.3328137397766113
3.3340134620666504
3.3308284282684326
3.324988842010498
3.325730800628662
3.326669692993164
3.3264946937561035
3.3280224800109863
3.326683282852173
3.3260059356689453
3.3256874084472656
3.325286388397217
3.3269824981689453
3.3216488361358643
3.3207831382751465
3.3204004764556885
3.3208651542663574
3.320456027984619
3.3194069862365723
3.320322036743164
3.321098804473877
3.320091724395752
3.3200302124023438
3.312793731689453
3.3138203620910645
3.3145155906677246
3.313361167907715
3.314589023590088
3.312743663787842
3.3152871131896973
3.3146023750305176
3.3144545555114746
3.3154616355895996
3.309864044189453
3.3084025382995605
3.309183120727539
3.3075480461120605
3.3081459999084473
3.3102145195007324
3.307755470275879
3.309384822845459
3.3099474906921387
3.3068346977233887
3.3037607669830322
3.303309917449951
3.3034653663635254
3.303149700164795
3.3032970428466797
3.3043971061706543
3.3046159744262695
3.3023345470428467
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 433, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 295, in margin_classifiers_perf
    l.backward()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt