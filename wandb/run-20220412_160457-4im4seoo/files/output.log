tensor([ 0.0299, -0.0021,  0.0006,  ..., -0.0017, -0.0017, -0.0046])
CMM train_err=0.0, err=45.439998626708984
1.13724946975708
1.0811086893081665
1.0302867889404297
0.9843729734420776
0.9429893493652344
0.9057832956314087
0.8724237084388733
0.8425971269607544
0.8160169720649719
0.7924038171768188
0.7715045213699341
0.7530719041824341
0.7368799448013306
0.7227137684822083
0.7103736996650696
0.6996691823005676
0.6904265880584717
0.6824807524681091
0.6756821870803833
0.669892430305481
0.6649834513664246
0.6608421802520752
0.6573619246482849
0.6544517874717712
0.652026891708374
0.6500166654586792
0.6483561992645264
0.6469886302947998
0.645868182182312
0.6449518203735352
0.6442055702209473
0.6435988545417786
0.6431072950363159
0.6427097320556641
0.642390251159668
0.6421319842338562
0.6419253349304199
0.6417590379714966
0.64162677526474
0.6415198445320129
0.6414350867271423
0.6413673162460327
0.6413130760192871
0.6412700414657593
0.6412358283996582
0.6412085294723511
0.6411871910095215
0.6411699652671814
0.6411558389663696
0.641144871711731
0.6411365270614624
0.6411298513412476
0.6411238312721252
0.6411198377609253
0.641116201877594
0.6411132216453552
0.641110897064209
0.6411096453666687
0.6411082148551941
0.6411066651344299
0.6411060690879822
0.6411052942276001
0.6411048173904419
0.6411043405532837
0.6411032676696777
0.6411032676696777
0.6411028504371643
0.6411030292510986
0.64110267162323
0.6411029100418091
0.6411029696464539
0.6411027312278748
0.6411024332046509
0.6411027908325195
0.6411026120185852
0.64110267162323
0.6411025524139404
0.6411024332046509
0.6411024928092957
tensor([ 0.0693, -0.0051,  0.0020,  ..., -0.0041, -0.0040, -0.0104],
       requires_grad=True)
w=1.0, train_err=0.0, err=45.36000061035156
====================l1=========================
CMM train_err=0.0, err=7.684999465942383
C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\sklearn\svm\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
3.832219362258911
3.815232038497925
3.8238253593444824
3.82572078704834
3.820128917694092
3.8194470405578613
3.823192596435547
3.8194127082824707
3.811528205871582
3.8222880363464355
3.81722354888916
3.8132214546203613
3.8281726837158203
3.8215084075927734
3.8282008171081543
3.8134756088256836
3.8269925117492676
3.803981065750122
3.8114354610443115
3.8218231201171875
3.806950092315674
3.815009593963623
3.8181228637695312
3.8302159309387207
3.8222479820251465
3.817896842956543
3.819671154022217
3.817368507385254
3.82389760017395
3.8303349018096924
3.8204026222229004
3.8125298023223877
3.819948434829712
3.8244309425354004
3.8138108253479004
3.825955867767334
3.8171920776367188
3.824831962585449
3.82319974899292
3.817976951599121
3.8232152462005615
3.815176248550415
3.817765712738037
3.8115668296813965
3.8217387199401855
3.8104186058044434
3.8087663650512695
3.805631637573242
3.8325347900390625
3.811030149459839
3.8143463134765625
3.8233375549316406
3.8027448654174805
3.8197476863861084
3.8194222450256348
3.823112964630127
3.8172993659973145
3.8211305141448975
3.821232795715332
3.8224411010742188
3.8138647079467773
3.8318352699279785
3.8117966651916504
3.8039045333862305
3.8142452239990234
3.8170697689056396
3.8213605880737305
3.8213796615600586
3.828176259994507
3.815676689147949
3.823305606842041
3.8248846530914307
3.8149006366729736
3.826852560043335
3.8335442543029785
3.815253973007202
3.8200013637542725
3.818943738937378
3.8150532245635986
3.8306665420532227
3.818542003631592
3.8191537857055664
3.8192756175994873
3.812004566192627
3.8215725421905518
3.818023443222046
3.8146021366119385
3.821580648422241
3.8220200538635254
3.8176584243774414
3.810119867324829
3.8157505989074707
3.8290319442749023
3.8255860805511475
3.8231165409088135
3.826439619064331
3.8256471157073975
3.8129498958587646
3.829967498779297
3.8172640800476074
3.8214492797851562
3.816387891769409
3.819593906402588
3.8172130584716797
3.830038070678711
3.808849573135376
3.834665060043335
3.820925235748291
3.8176026344299316
3.819222927093506
3.82012939453125
3.81939435005188
3.816969156265259
3.821610927581787
3.807302951812744
3.808149814605713
3.8173415660858154
3.815016984939575
3.8223044872283936
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 434, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 296, in margin_classifiers_perf
    l.backward()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt