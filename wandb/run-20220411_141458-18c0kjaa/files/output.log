tensor([ 0.0316, -0.0013,  0.0054,  ...,  0.0014, -0.0015,  0.0022])
CMM train_err=0.0, err=45.025001525878906
770273.3125
115.81135559082031
1427.073974609375
13969.390625
964.052490234375
509.38751220703125
10733849.0
160.95228576660156
4900.03515625
2989.119384765625
45.48783493041992
901839.5625
8489.4033203125
27042.552734375
1091.4677734375
854436.0625
389.21826171875
613970176.0
1263317.875
4910.5205078125
2085.11572265625
1236.975830078125
11104.7158203125
304.9563293457031
482649.5625
26233.8046875
1916.0870361328125
1087.716796875
178.4815673828125
503229.53125
91041.3046875
19566.587890625
4909.20556640625
1415.53076171875
465.361328125
171.76950073242188
73.23058319091797
56.7210578918457
27.416446685791016
14.415616989135742
9.452569961547852
7.048958778381348
4.638506889343262
3.2182817459106445
2.515237808227539
1.9252842664718628
2.054072380065918
1.7017370462417603
1.4591706991195679
1.284986972808838
1.160036563873291
1.0682965517044067
0.9995461106300354
0.9470702409744263
0.9063565731048584
0.8743011951446533
0.8487269878387451
0.8280802965164185
0.8112328052520752
0.7973523139953613
0.7858162522315979
0.776153564453125
0.7680026888847351
0.7610825300216675
0.7551740407943726
0.7501032948493958
0.745730459690094
0.7419427633285522
0.7386515140533447
0.7357797622680664
0.7332676649093628
0.7310625910758972
0.7291236519813538
0.7274143099784851
0.7259049415588379
0.7245676517486572
0.7233824729919434
0.7223304510116577
0.7213956117630005
0.7205631136894226
0.7198207378387451
0.7191586494445801
0.7185670137405396
0.718039870262146
0.7175678610801697
0.7171456813812256
0.7167671322822571
0.7164284586906433
0.7161248922348022
0.7158529162406921
0.715608537197113
0.7153897881507874
0.7151933908462524
0.7150164842605591
0.7148584127426147
0.7147154808044434
0.7145874500274658
0.7144728899002075
0.7143697142601013
0.7142770290374756
0.71419358253479
0.7141185998916626
0.7140515446662903
0.7139906883239746
0.7139362096786499
0.7138874530792236
0.7138429880142212
0.7138035297393799
0.7137675881385803
0.7137356996536255
0.7137063145637512
0.7136800289154053
0.7136566638946533
0.7136358022689819
0.713617205619812
0.7136003971099854
0.7135852575302124
0.71357262134552
0.7135616540908813
0.7135521173477173
0.7135440707206726
0.7135374546051025
0.7135321497917175
0.7135277986526489
0.7135241031646729
0.7135213613510132
0.7135189175605774
0.7135173678398132
0.7135158181190491
0.7135146856307983
0.7135137319564819
0.7135128974914551
0.7135123014450073
0.7135118246078491
0.7135114073753357
0.7135111093521118
0.7135109901428223
0.7135107517242432
0.7135104537010193
0.7135103940963745
0.7135101556777954
0.7135100364685059
0.7135100364685059
0.7135099172592163
0.7135099172592163
0.7135098576545715
0.7135098576545715
0.7135097980499268
0.7135097980499268
0.7135097980499268
0.7135097980499268
0.7135097980499268
0.713509738445282
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
0.7135096788406372
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin.py", line 427, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin.py", line 220, in margin_classifiers_perf
    l.backward()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt