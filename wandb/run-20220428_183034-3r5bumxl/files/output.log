C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\sklearn\svm\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
CMM train_err=0.0, err=5.029999732971191
2.8936753273010254
2.8739545345306396
2.8590710163116455
2.8465189933776855
2.8354833126068115
2.825556993484497
2.816495180130005
2.8081350326538086
2.8003604412078857
2.793083906173706
2.7862377166748047
2.7797694206237793
2.773634672164917
2.767798900604248
2.762230396270752
2.756904363632202
2.7517993450164795
2.746896266937256
2.742177724838257
2.7376298904418945
2.73323917388916
2.7289950847625732
2.724886178970337
2.7209036350250244
2.717039108276367
2.713284730911255
2.7096335887908936
2.7060794830322266
2.7026166915893555
2.699240207672119
2.6959445476531982
2.692725658416748
2.6895787715911865
2.6865010261535645
2.683488130569458
2.6805365085601807
2.6776440143585205
2.674807071685791
2.6720235347747803
2.669290065765381
2.6666057109832764
2.6639673709869385
2.6613729000091553
2.658820867538452
2.656309127807617
2.653836250305176
2.651400566101074
2.649000406265259
2.646634817123413
2.6443018913269043
2.6420013904571533
2.639730930328369
2.6374902725219727
2.63527774810791
2.6330926418304443
2.6309335231781006
2.628800392150879
2.6266918182373047
2.6246073246002197
2.6225457191467285
2.6205062866210938
2.6184892654418945
2.61649227142334
2.614516019821167
2.6125593185424805
2.6106221675872803
2.60870361328125
2.6068027019500732
2.604919195175171
2.6030526161193848
2.601203680038452
2.599370002746582
2.5975522994995117
2.595750093460083
2.5939621925354004
2.592189073562622
2.59043025970459
2.5886850357055664
2.58695387840271
2.585235595703125
2.583529472351074
2.5818369388580322
2.580156087875366
2.5784871578216553
2.576829671859741
2.575183629989624
2.573549270629883
2.571925640106201
2.570312738418579
2.5687098503112793
2.567117929458618
2.565535545349121
2.5639634132385254
2.5624003410339355
2.560847282409668
2.5593032836914062
2.5577683448791504
2.5562422275543213
2.554725170135498
2.5532162189483643
2.551715850830078
2.5502238273620605
2.5487396717071533
2.5472638607025146
2.5457956790924072
2.54433536529541
2.5428824424743652
2.5414364337921143
2.5399985313415527
2.538567304611206
2.5371434688568115
2.5357260704040527
2.534315824508667
2.532912015914917
2.531515121459961
2.5301246643066406
2.528740644454956
2.5273633003234863
2.525991439819336
2.5246260166168213
2.523266553878784
2.5219130516052246
2.5205655097961426
2.519223690032959
2.5178871154785156
2.516556739807129
2.5152313709259033
2.513911724090576
2.5125973224639893
2.5112884044647217
2.5099847316741943
2.508686065673828
2.507392168045044
2.506103754043579
2.504819869995117
2.5035414695739746
2.502267599105835
2.500998020172119
2.4997336864471436
2.498473644256592
2.497218370437622
2.4959678649902344
2.4947211742401123
2.4934794902801514
2.4922420978546143
2.491008996963501
2.4897801876068115
2.4885551929473877
2.487334966659546
2.4861180782318115
2.48490571975708
2.4836976528167725
2.4824931621551514
2.481292724609375
2.4800963401794434
2.4789037704467773
2.477714776992798
2.476529836654663
2.475348711013794
2.4741711616516113
2.472996473312378
2.4718260765075684
2.4706597328186035
2.469496011734009
2.468336343765259
2.4671802520751953
2.46602725982666
2.4648778438568115
2.4637317657470703
2.4625892639160156
2.46144962310791
2.460313320159912
2.459181070327759
2.4580509662628174
2.4569246768951416
2.455801248550415
2.454680919647217
2.453563690185547
2.4524502754211426
2.451338291168213
2.4502310752868652
2.4491262435913086
2.4480233192443848
2.446925401687622
2.445828676223755
2.444735288619995
2.4436450004577637
2.4425575733184814
2.4414730072021484
2.4403910636901855
2.4393115043640137
2.4382357597351074
2.437161684036255
2.4360907077789307
2.4350223541259766
2.4339568614959717
2.432893991470337
2.4318337440490723
2.4307758808135986
2.429720401763916
2.4286680221557617
2.4276182651519775
2.426570415496826
2.425525426864624
2.424483060836792
2.423442840576172
2.42240571975708
2.421370029449463
2.420337677001953
2.419307231903076
2.4182794094085693
2.4172537326812744
2.4162304401397705
2.4152095317840576
2.4141910076141357
2.413174867630005
2.412160634994507
2.411149024963379
2.410139560699463
2.409132242202759
2.408127546310425
2.4071245193481445
2.4061241149902344
2.405125617980957
2.404129981994629
2.403135299682617
2.4021434783935547
2.401153802871704
2.4001660346984863
2.3991806507110596
2.3981971740722656
2.3972153663635254
2.3962364196777344
2.395259380340576
2.3942837715148926
2.393310546875
2.3923397064208984
2.3913700580596924
2.3904032707214355
2.3894379138946533
2.388474702835083
2.3875136375427246
2.38655424118042
2.385596752166748
2.384641170501709
2.383687973022461
2.3827357292175293
2.381786346435547
2.380838394165039
2.379892349243164
2.378948211669922
2.3780064582824707
2.377065420150757
2.376127004623413
2.375190019607544
2.3742549419403076
2.373322010040283
2.3723909854888916
2.3714609146118164
2.3705332279205322
2.3696072101593018
2.368682622909546
2.367760181427002
2.3668394088745117
2.365920305252075
2.365002393722534
2.3640875816345215
2.363173246383667
2.3622612953186035
2.3613505363464355
2.3604419231414795
2.35953426361084
2.358628988265991
2.3577256202697754
2.3568227291107178
2.3559224605560303
2.3550233840942383
2.3541259765625
2.3532299995422363
2.3523364067077637
2.3514435291290283
2.350552797317505
2.349663496017456
2.348775863647461
2.3478896617889404
2.3470048904418945
2.3461220264434814
2.3452401161193848
2.344360828399658
2.343482494354248
2.3426051139831543
2.3417298793792725
2.3408560752868652
2.3399839401245117
2.3391127586364746
2.3382439613342285
2.3373754024505615
2.3365092277526855
2.3356447219848633
2.3347811698913574
2.333919048309326
2.3330585956573486
2.332200288772583
2.3313422203063965
2.3304860591888428
2.3296313285827637
2.328778028488159
2.3279261589050293
2.327075719833374
2.3262269496917725
2.3253791332244873
2.324533224105835
2.323688507080078
2.3228445053100586
2.32200288772583
2.321162700653076
2.3203227519989014
2.3194849491119385
2.318648338317871
2.3178133964538574
2.31697940826416
2.3161470890045166
2.3153164386749268
2.314486026763916
2.313657760620117
2.3128304481506348
2.312004804611206
2.3111801147460938
2.310357093811035
2.3095357418060303
2.3087148666381836
2.3078954219818115
2.3070781230926514
2.306260824203491
2.305445671081543
2.3046317100524902
2.303818941116333
2.303006887435913
2.302196741104126
2.3013882637023926
2.3005800247192383
2.2997734546661377
2.2989683151245117
2.2981646060943604
2.297361135482788
2.296560049057007
2.295759677886963
2.2949602603912354
2.2941625118255615
2.293365716934204
2.2925703525543213
2.291776180267334
2.290983200073242
2.290191650390625
2.289400577545166
2.28861141204834
2.28782320022583
2.287036180496216
2.2862493991851807
2.2854652404785156
2.284681558609009
2.2838993072509766
2.28311824798584
2.2823383808135986
2.2815589904785156
2.2807812690734863
2.2800049781799316
2.2792298793792725
2.2784554958343506
2.277682065963745
2.2769100666046143
2.276139259338379
2.27536940574646
2.2746009826660156
2.2738335132598877
2.273066997528076
2.272301197052002
2.2715373039245605
2.2707743644714355
2.270012378692627
2.2692513465881348
2.2684922218322754
2.267733335494995
2.266975164413452
2.266219139099121
2.2654638290405273
2.264709234237671
2.263956069946289
2.2632040977478027
2.2624528408050537
2.261702537536621
2.260953664779663
2.2602059841156006
2.2594590187072754
2.2587132453918457
2.2579684257507324
2.2572250366210938
2.2564821243286133
2.255740165710449
2.255000114440918
2.254260778427124
2.253521680831909
2.252784490585327
2.2520482540130615
2.251312494277954
2.2505781650543213
2.249844789505005
2.249112367630005
2.2483813762664795
2.247650623321533
2.2469210624694824
2.2461934089660645
2.2454657554626465
2.244739294052124
2.244014263153076
2.2432897090911865
2.2425663471221924
2.2418437004089355
2.2411224842071533
2.2404019832611084
2.239682674407959
2.2389638423919678
2.2382466793060303
2.23753023147583
2.236814498901367
2.236100196838379
2.2353861331939697
2.2346737384796143
2.233961820602417
2.2332510948181152
2.232541561126709
2.23183274269104
2.231123924255371
2.230417490005493
2.2297117710113525
2.229006290435791
2.228302001953125
2.2275993824005127
2.2268970012664795
2.2261950969696045
2.225494861602783
2.2247955799102783
2.2240967750549316
2.2233989238739014
2.2227022647857666
2.222006320953369
2.2213118076324463
2.2206177711486816
2.219924211502075
2.2192318439483643
2.218540906906128
2.217850685119629
2.217160701751709
2.216472625732422
2.215785026550293
2.2150979042053223
2.214411735534668
2.2137269973754883
2.213042974472046
2.2123591899871826
2.211676836013794
2.210995674133301
2.210314989089966
2.209634780883789
2.208955764770508
2.208278179168701
2.2076010704040527
2.2069249153137207
2.2062489986419678
2.2055745124816895
2.2049009799957275
2.204227924346924
2.2035555839538574
2.2028841972351074
2.2022147178649902
2.201545000076294
2.200875997543335
2.2002081871032715
2.1995415687561035
2.1988754272460938
2.198209524154663
2.1975455284118652
2.1968820095062256
2.196218729019165
2.195556640625
2.1948959827423096
2.1942358016967773
2.193575859069824
2.192917585372925
2.1922595500946045
2.1916024684906006
2.190946340560913
2.190290927886963
2.18963623046875
2.1889824867248535
2.1883296966552734
2.1876771450042725
2.187025785446167
2.1863749027252197
2.185725688934326
2.1850767135620117
2.1844282150268555
2.1837809085845947
2.1831347942352295
2.1824886798858643
2.1818432807922363
2.181199312210083
2.180555820465088
2.179913282394409
2.1792709827423096
2.1786298751831055
2.177989959716797
2.1773502826690674
2.176711320877075
2.1760733127593994
2.175436496734619
2.174799919128418
2.174163818359375
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_average_loss.py", line 456, in <module>
    margin_classifiers_perf(d=100,n=1000,approx_tau=1, SNR=10, n_test=1e4, s=2, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_average_loss.py", line 230, in margin_classifiers_perf
    l.backward()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\__init__.py", line 166, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\__init__.py", line 68, in _make_grads
    new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))
KeyboardInterrupt