tensor([ 0.0152, -0.0029, -0.0008,  ...,  0.0013, -0.0007, -0.0010])
CMM train_err=0.0, err=46.869998931884766
1.076495885848999
1.01636803150177
0.9616421461105347
0.9119127988815308
0.8668075799942017
0.8259718418121338
0.7890856862068176
0.7558408379554749
0.7259570956230164
0.6991648077964783
0.6752153038978577
0.6538770794868469
0.6349273920059204
0.6181600093841553
0.6033803224563599
0.5904045701026917
0.579059898853302
0.5691839456558228
0.5606253147125244
0.5532429218292236
0.5469036102294922
0.5414854288101196
0.5368772745132446
0.5329768061637878
0.5296899676322937
0.5269336104393005
0.5246323347091675
0.5227190852165222
0.5211348533630371
0.5198294520378113
0.5187572240829468
0.5178795456886292
0.5171641111373901
0.5165820121765137
0.5161107778549194
0.5157299041748047
0.5154227018356323
0.5151754021644592
0.514977216720581
0.5148181319236755
0.5146917700767517
0.5145901441574097
0.5145093202590942
0.5144453644752502
0.5143942832946777
0.5143535137176514
0.514320969581604
0.5142956972122192
0.514275312423706
0.514259397983551
0.5142468214035034
0.514236569404602
0.514228880405426
0.5142231583595276
0.5142178535461426
0.5142138600349426
0.5142107009887695
0.5142080783843994
0.5142062902450562
0.514204740524292
0.5142036080360413
0.5142027735710144
0.5142019987106323
0.5142016410827637
0.5142013430595398
0.5142008066177368
0.5142005085945129
0.5142002105712891
0.5141997933387756
0.5141996741294861
0.5141996145248413
0.5141996741294861
0.5141996145248413
0.5141996145248413
0.5141993165016174
0.5141996145248413
0.5141996741294861
tensor([ 0.0445, -0.0085, -0.0025,  ...,  0.0039, -0.0024, -0.0029],
       requires_grad=True)
w=1.0, train_err=0.0, err=46.77000045776367
====================l1=========================
CMM train_err=0.0, err=13.335000038146973
C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\sklearn\svm\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
3.3073394298553467
3.0913634300231934
3.040741443634033
3.0210251808166504
3.0128111839294434
3.0073280334472656
3.0045742988586426
3.0022501945495605
3.0011117458343506
2.999408006668091
2.99849271774292
2.996919631958008
2.996580123901367
2.9959652423858643
2.995372772216797
2.994894504547119
2.9947023391723633
2.9943394660949707
2.994137763977051
2.993709087371826
2.993647575378418
2.9933383464813232
2.9934260845184326
2.99310040473938
2.9928359985351562
2.9927902221679688
2.992628812789917
2.9923787117004395
2.9924087524414062
2.9926438331604004
2.9926469326019287
2.991816759109497
2.9927263259887695
2.992126226425171
2.992067813873291
2.992278575897217
2.992558240890503
2.9919869899749756
2.9923579692840576
2.9921488761901855
2.9918901920318604
2.99218487739563
2.9914379119873047
2.992342472076416
2.991696357727051
2.9918150901794434
2.991600513458252
2.9922051429748535
2.9918227195739746
2.9918289184570312
2.9923577308654785
2.9921875
2.992058753967285
2.991758346557617
2.9917078018188477
2.991799831390381
2.991950750350952
2.9920754432678223
2.9918856620788574
2.991710662841797
2.991637706756592
2.9918558597564697
2.9919161796569824
2.9919800758361816
2.992058515548706
2.9913394451141357
2.991731643676758
2.9914650917053223
2.991713762283325
2.9915122985839844
2.9922070503234863
2.991786003112793
2.9913978576660156
2.9914660453796387
2.9914133548736572
2.9917078018188477
2.991933822631836
2.9914064407348633
2.9922091960906982
2.9914722442626953
2.991373062133789
2.9917428493499756
2.9916129112243652
2.991929769515991
2.9918036460876465
2.9913363456726074
2.991772174835205
2.9913806915283203
2.9918594360351562
2.991185426712036
2.991715669631958
2.9913721084594727
2.9914820194244385
2.9914772510528564
2.991946220397949
2.991565704345703
2.991766929626465
2.9917922019958496
2.9916176795959473
2.9912219047546387
w=1.0, train_err=0.0, err=8.164999961853027