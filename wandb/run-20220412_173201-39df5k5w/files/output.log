tensor([ 0.0148,  0.0030,  0.0005,  ..., -0.0030, -0.0010,  0.0029])
CMM train_err=0.0, err=46.51499938964844
1.0780820846557617
1.018056035041809
0.9634334444999695
0.9138066172599792
0.8688009977340698
0.8280655741691589
0.791273832321167
0.7581239938735962
0.7283282279968262
0.7016236782073975
0.6777596473693848
0.6565018892288208
0.6376304030418396
0.6209371089935303
0.6062265038490295
0.5933172702789307
0.5820330381393433
0.5722134709358215
0.5637071132659912
0.5563704967498779
0.5500732660293579
0.5446935892105103
0.5401189923286438
0.536247730255127
0.5329867601394653
0.5302525162696838
0.5279693603515625
0.5260726809501648
0.5245029926300049
0.5232081413269043
0.5221452713012695
0.5212756395339966
0.5205658674240112
0.5199894905090332
0.5195220112800598
0.5191442966461182
0.5188401937484741
0.5185952186584473
0.5183985233306885
0.5182409882545471
0.5181154608726501
0.5180149078369141
0.5179345607757568
0.5178706645965576
0.5178198218345642
0.5177798271179199
0.5177473425865173
0.5177221298217773
0.5177019834518433
0.5176866054534912
0.5176737904548645
0.5176637172698975
0.5176563262939453
0.5176496505737305
0.5176445245742798
0.5176408886909485
0.5176378488540649
0.5176357626914978
0.5176334381103516
0.5176321268081665
0.5176308155059814
0.5176296830177307
0.5176290273666382
0.5176286101341248
0.5176278352737427
0.5176277160644531
0.5176272988319397
0.5176273584365845
0.5176271200180054
0.5176265835762024
0.5176268219947815
0.5176264643669128
0.5176264643669128
0.5176264047622681
0.5176265239715576
0.5176264047622681
C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\sklearn\svm\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
0.5176263451576233
tensor([ 0.0432,  0.0090,  0.0011,  ..., -0.0090, -0.0028,  0.0082],
       requires_grad=True)
w=1.0, train_err=0.0, err=46.38999938964844
====================l1=========================
CMM train_err=0.0, err=8.885000228881836
3.236328601837158
3.1253771781921387
3.0992751121520996
3.090773820877075
3.0864763259887695
3.0839006900787354
3.081803798675537
3.07987642288208
3.0790505409240723
3.0786170959472656
3.0773937702178955
3.077599048614502
3.0770153999328613
3.0765774250030518
3.0765209197998047
3.076098918914795
3.0761208534240723
3.075714111328125
3.0763955116271973
3.0754942893981934
3.0763607025146484
3.0752599239349365
3.0750911235809326
3.075375556945801
3.075319290161133
3.0757884979248047
3.0751523971557617
3.074812412261963
3.0745596885681152
3.074817180633545
3.0747904777526855
3.0749950408935547
3.075080394744873
3.0746960639953613
3.0745351314544678
3.0745368003845215
3.074812412261963
3.0744705200195312
3.074638843536377
3.074490547180176
3.0746049880981445
3.0746688842773438
3.0745344161987305
3.074038505554199
3.0743274688720703
3.0744423866271973
3.0743277072906494
3.07485032081604
3.074007987976074
3.0742344856262207
3.074498176574707
3.0748496055603027
3.074355125427246
3.074531078338623
3.073984146118164
3.074669361114502
3.0747666358947754
3.074777126312256
3.0739927291870117
3.0744266510009766
3.0742149353027344
3.074404239654541
3.074566602706909
3.0744290351867676
3.074521064758301
3.074808359146118
3.0743706226348877
3.0742647647857666
3.074042320251465
3.0749478340148926
3.0743675231933594
3.074014902114868
3.0745439529418945
3.07450008392334
3.0741000175476074
3.074188232421875
3.0744001865386963
3.0744147300720215
3.074467182159424
3.074153423309326
3.074436902999878
3.0738680362701416
3.074258804321289
3.0743184089660645
3.0744709968566895
3.074075222015381
3.0743625164031982
3.0742125511169434
3.0747649669647217
3.074387550354004
3.0744359493255615
3.0745158195495605
3.0740294456481934
3.0739054679870605
3.074540376663208
3.0743560791015625
3.0743370056152344
3.0744171142578125
3.07411789894104
3.0742757320404053
w=1.0, train_err=0.0, err=3.805000066757202