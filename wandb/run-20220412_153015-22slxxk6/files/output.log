tensor([ 0.0328,  0.0043, -0.0002,  ...,  0.0004,  0.0013, -0.0036])
CMM train_err=0.0, err=44.54999923706055
1.1410455703735352
1.0851495265960693
1.03456449508667
0.9888828992843628
0.9477261304855347
0.9107402563095093
0.8775926828384399
0.847973108291626
0.8215889930725098
0.7981644868850708
0.7774450778961182
0.7591830492019653
0.7431533336639404
0.7291383743286133
0.7169397473335266
0.7063665986061096
0.6972448825836182
0.6894093155860901
0.6827124953269958
0.677013635635376
0.672187328338623
0.6681188941001892
0.6647034883499146
0.661849319934845
0.6594753265380859
0.657508373260498
0.6558842062950134
0.6545497179031372
0.6534560322761536
0.6525636315345764
0.651837170124054
0.6512465476989746
0.6507694721221924
0.6503835916519165
0.650073230266571
0.6498235464096069
0.6496235728263855
0.6494631767272949
0.6493346691131592
0.6492318511009216
0.6491503715515137
0.6490854024887085
0.64903324842453
0.6489920020103455
0.6489591598510742
0.6489333510398865
0.6489123106002808
0.6488961577415466
0.6488828659057617
0.6488724946975708
0.6488648056983948
0.6488579511642456
0.6488533020019531
0.6488486528396606
0.6488456130027771
0.6488430500030518
0.6488409042358398
0.6488392353057861
0.6488379240036011
0.6488369703292847
0.6488362550735474
0.6488355398178101
0.6488348245620728
0.6488345861434937
0.6488347053527832
0.6488341689109802
0.6488339900970459
0.6488338112831116
0.6488334536552429
0.6488333344459534
0.6488333940505981
0.6488332152366638
0.6488333344459534
0.6488333344459534
0.6488330960273743
0.6488333344459534
C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\sklearn\svm\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
0.648833155632019
tensor([ 0.0758,  0.0101, -0.0004,  ...,  0.0013,  0.0034, -0.0083],
       requires_grad=True)
w=1.0, train_err=0.0, err=44.40500259399414
====================l1=========================
CMM train_err=0.0, err=6.899999618530273
3.6640501022338867
3.653481960296631
3.6543736457824707
3.6547181606292725
3.660952091217041
3.6606171131134033
3.6524481773376465
3.667994499206543
3.673292636871338
3.659658432006836
3.6543586254119873
3.6494140625
3.6621785163879395
3.659078598022461
3.6582937240600586
3.668872356414795
3.652355194091797
3.6542797088623047
3.66082763671875
3.6492815017700195
3.6699976921081543
3.670917510986328
3.669825553894043
3.6473827362060547
3.650554656982422
3.6596386432647705
3.666968822479248
3.660708427429199
3.654615879058838
3.653970241546631
3.666515588760376
3.6553401947021484
3.660001516342163
3.6547937393188477
3.656318426132202
3.6658036708831787
3.6477861404418945
3.661686897277832
3.664947509765625
3.6696431636810303
3.6572039127349854
3.6639037132263184
3.6682257652282715
3.6620521545410156
3.663731098175049
3.658883810043335
3.6674795150756836
3.6527957916259766
3.6597423553466797
3.658047676086426
3.669171094894409
3.664355754852295
3.6537222862243652
3.6667282581329346
3.671889543533325
3.649810314178467
3.6639370918273926
3.660588502883911
3.665416717529297
3.670597553253174
3.6645126342773438
3.6643550395965576
3.666790246963501
3.6548540592193604
3.656411647796631
3.6610405445098877
3.6617562770843506
3.655609130859375
3.6613950729370117
3.6580653190612793
3.6576101779937744
3.643841028213501
3.6619791984558105
3.660121202468872
3.6666979789733887
3.66365647315979
3.658020496368408
3.663461685180664
3.6545348167419434
3.6528964042663574
3.6624631881713867
3.6596672534942627
3.6581039428710938
3.6750733852386475
3.668656349182129
3.650473117828369
3.649631977081299
3.658851146697998
3.663484573364258
3.6701953411102295
3.6627249717712402
3.664292335510254
3.664987564086914
3.6538920402526855
3.657715320587158
3.6560330390930176
3.6543073654174805
3.6523900032043457
3.6553308963775635
3.6571664810180664
3.651181697845459
3.65974760055542
3.651641845703125
3.6633872985839844
3.6580193042755127
3.653203010559082
3.6606557369232178
3.669504165649414
3.658384323120117
3.6567189693450928
3.667825698852539
3.6652345657348633
3.6618964672088623
3.6717913150787354
3.6573314666748047
3.661712169647217
3.6458091735839844
3.662153720855713
3.6579298973083496
3.662764072418213
3.661731243133545
3.6571149826049805
3.6580567359924316
3.6635260581970215
3.6609034538269043
3.663470506668091
3.6475272178649902
3.657672643661499
3.6614081859588623
3.6487910747528076
3.6596436500549316
3.650611400604248
3.657219886779785
3.6717398166656494
3.655026435852051
3.660550117492676
3.6484107971191406
3.657456398010254
3.6590895652770996
3.6683545112609863
3.6497652530670166
3.660065174102783
3.6648736000061035
3.6558797359466553
3.662618637084961
3.6613903045654297
3.671438217163086
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 434, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 297, in margin_classifiers_perf
    optim.step()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\optim\optimizer.py", line 87, in wrapper
    with torch.autograd.profiler.record_function(profile_name):
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\profiler.py", line 436, in __enter__
    self.handle = torch.ops.profiler._record_function_enter(self.name, self.args)
KeyboardInterrupt