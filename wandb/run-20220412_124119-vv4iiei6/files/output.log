tensor([ 0.0314, -0.0012,  0.0019,  ...,  0.0020, -0.0001,  0.0044])
CMM train_err=0.0, err=44.7400016784668
1.1394935846328735
1.0834846496582031
1.0327856540679932
0.9869917631149292
0.9457206726074219
0.9086184501647949
0.8753576278686523
0.8456224203109741
0.8191249370574951
0.7955875396728516
0.7747541666030884
0.756382405757904
0.7402439713478088
0.7261247634887695
0.7138239145278931
0.703153669834137
0.6939374804496765
0.6860140562057495
0.6792342066764832
0.6734578609466553
0.6685599088668823
0.6644258499145508
0.660951554775238
0.6580444574356079
0.6556231379508972
0.6536139249801636
0.6519527435302734
0.6505856513977051
0.649463415145874
0.6485459208488464
0.6477979421615601
0.647189736366272
0.6466974020004272
0.6462988257408142
0.6459768414497375
0.6457177400588989
0.6455092430114746
0.6453424692153931
0.6452078223228455
0.6451018452644348
0.6450152397155762
0.644947350025177
0.6448922753334045
0.6448484063148499
0.6448132991790771
0.6447858214378357
0.6447641849517822
0.6447464823722839
0.6447324752807617
0.6447213888168335
0.6447128057479858
0.6447053551673889
0.64469975233078
0.6446954011917114
0.6446917057037354
0.64468914270401
0.6446866393089294
0.6446847319602966
0.6446837186813354
0.6446822285652161
0.6446815133094788
0.6446806192398071
0.6446802616119385
0.6446794867515564
0.6446788907051086
0.644679069519043
0.6446788311004639
0.6446786522865295
0.6446783542633057
0.6446780562400818
0.6446777582168579
0.6446781158447266
0.6446778774261475
0.6446781158447266
0.6446777582168579
0.6446778178215027
0.6446778774261475
0.6446778774261475
0.6446778178215027
tensor([ 0.0730, -0.0028,  0.0036,  ...,  0.0049, -0.0005,  0.0105],
       requires_grad=True)
w=1.0, train_err=0.0, err=44.775001525878906
====================l1=========================
CMM train_err=0.0, err=6.824999809265137
3.3874247074127197
3.2433371543884277
3.1834850311279297
3.1597883701324463
3.150740146636963
3.147071599960327
3.1455841064453125
3.144996404647827
3.144711971282959
3.1446051597595215
3.144613265991211
3.1446170806884766
3.1446192264556885
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
3.144619941711426
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 433, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 294, in margin_classifiers_perf
    l = loss_average_poly(w, b, z1s, z2s, n1, n2) + torch.norm(w, p=1)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 26, in loss_average_poly
    return (torch.sum(1./(z1s @ v)) + b * torch.sum(1./(z2s @ v))) /(n1+b*n2)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\_tensor.py", line 26, in wrapped
    @functools.wraps(f, assigned=assigned)
KeyboardInterrupt