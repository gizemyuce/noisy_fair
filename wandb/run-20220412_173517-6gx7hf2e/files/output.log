tensor([ 0.0059,  0.0005, -0.0003,  ..., -0.0002, -0.0006, -0.0005])
CMM train_err=0.0, err=47.89500045776367
1.0191540718078613
0.9550892114639282
0.8964744806289673
0.8429024815559387
0.7940027117729187
0.7494235038757324
0.7088409662246704
0.6719577312469482
0.6384923458099365
0.6081909537315369
0.5808095932006836
0.5561280846595764
0.5339320302009583
0.51402747631073
0.49622946977615356
0.48036521673202515
0.4662743806838989
0.453800767660141
0.4428021311759949
0.43314293026924133
0.42469632625579834
0.4173407554626465
0.41096407175064087
0.4054630398750305
0.400737464427948
0.3966987133026123
0.3932625651359558
0.39035409688949585
0.3879026770591736
0.38584670424461365
0.3841301202774048
0.3827033042907715
0.38152170181274414
0.3805479407310486
0.37974807620048523
0.3790934979915619
0.37855982780456543
0.3781261146068573
0.3777744770050049
0.3774905204772949
0.37726157903671265
0.37707722187042236
0.37692904472351074
0.37681108713150024
0.37671583890914917
0.3766409754753113
0.37658098340034485
0.3765326142311096
0.3764946758747101
0.37646472454071045
0.37644049525260925
0.37642142176628113
0.37640661001205444
0.3763948082923889
0.37638503313064575
0.3763779401779175
0.37637192010879517
0.3763672709465027
0.37636375427246094
0.3763607442378998
0.37635865807533264
0.37635672092437744
0.3763550817966461
0.37635427713394165
0.3763532340526581
0.3763527572154999
0.37635231018066406
0.3763517737388611
0.376351535320282
0.37635117769241333
0.37635067105293274
0.37635067105293274
0.376350075006485
0.3763500452041626
0.3763500452041626
0.3763498067855835
0.37634992599487305
0.37634986639022827
0.37635016441345215
C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\sklearn\svm\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
0.3763499855995178
tensor([ 0.0235,  0.0020, -0.0011,  ..., -0.0009, -0.0021, -0.0019],
       requires_grad=True)
w=1.0, train_err=0.0, err=47.87000274658203
====================l1=========================
CMM train_err=0.0, err=8.045000076293945
3.1654248237609863
3.0779337882995605
3.064469337463379
3.059074878692627
3.0548391342163086
3.052471399307251
3.0508124828338623
3.048712730407715
3.0480542182922363
3.046691417694092
3.0463051795959473
3.045886754989624
3.0456857681274414
3.0447921752929688
3.045121908187866
3.044240951538086
3.044111728668213
3.0431406497955322
3.042875051498413
3.0432188510894775
3.0428223609924316
3.043239116668701
3.0425617694854736
3.0433201789855957
3.042419910430908
3.0424296855926514
3.04219913482666
3.04276180267334
3.042639970779419
3.0425424575805664
3.042177677154541
3.0420703887939453
3.042921543121338
3.0423192977905273
3.0427651405334473
3.04207444190979
3.042147636413574
3.0416455268859863
3.042422294616699
3.0412042140960693
3.041748046875
3.0415401458740234
3.0416817665100098
3.0412867069244385
3.0419094562530518
3.0423784255981445
3.0419135093688965
3.0426859855651855
3.042145252227783
3.041551113128662
3.0414185523986816
3.041853427886963
3.0421371459960938
3.0414319038391113
3.041193723678589
3.0420138835906982
3.0419530868530273
3.0421204566955566
3.04150652885437
3.0418996810913086
3.0426666736602783
3.0409979820251465
3.0415945053100586
3.0415899753570557
3.0417566299438477
3.041525363922119
3.041398525238037
3.0418338775634766
3.0419273376464844
3.042036533355713
3.042194366455078
3.0421624183654785
3.0424394607543945
3.0417017936706543
3.042067050933838
3.0425291061401367
3.0421628952026367
3.0416386127471924
3.041778087615967
3.0410828590393066
3.042759895324707
3.041991710662842
3.0416245460510254
3.0420429706573486
3.041841983795166
3.0416016578674316
3.041708469390869
3.0408613681793213
3.041684150695801
3.0420081615448
3.0420982837677
3.0418949127197266
3.0416481494903564
3.0418872833251953
3.041325569152832
3.042117118835449
3.041954278945923
3.0417654514312744
3.042039394378662
w=1.0, train_err=0.0, err=4.154999732971191