====================l1=========================
CMM train_err=0.0, err=0.8100000023841858









5.5827999114990235.4087009429931645.2757215499877935.1706047058105475.0853481292724615.014807701110844.95549726486206054.9049706459045414.8614559173583984.8236279487609864.7904801368713384.7612314224243164.7352671623229984.7120943069458014.691313266754154.6725997924804694.6556777954101564.6403274536132814.62635803222656254.6136054992675784.601933956146244.5912270545959474.5813803672790534.5723075866699224.5639305114746094.5561828613281254.5490045547485354.5423445701599124.53615283966064454.5303912162780764.5250229835510254.5200123786926274.5153360366821294.510960578918457
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\sparse_gt_variance.py", line 430, in <module>
    margin_classifiers_perf(d=2,n=100,approx_tau=1, SNR=10, n_test=1e4, s=1, random_flip_prob=0, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\sparse_gt_variance.py", line 228, in margin_classifiers_perf
    optim.step()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\optim\sgd.py", line 135, in step
    params_with_grad.append(p)
KeyboardInterrupt