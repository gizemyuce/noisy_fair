tensor([ 2.8609e-02, -1.8377e-03, -1.5499e-03,  ..., -3.0721e-03,
        -4.6125e-05, -3.4557e-03])
CMM train_err=0.0, err=45.79999923706055
806.085205078125
362.64251708984375
428280.6875
192346.28125
86385.296875
38796.75390625
17424.1015625
7825.40478515625
3514.501220703125
1580.646240234375
709.9086303710938
318.833984375
143.30184936523438
64.41633605957031
699.2528076171875
314.74835205078125
141.4783477783203
63.62767028808594
31.469825744628906
14.485239028930664
170430.0625
76542.3359375
34376.11328125
15438.7587890625
6933.7724609375
3114.980712890625
1398.873046875
628.31884765625
282.2139892578125
129.17893981933594
58.303428649902344
64.76097106933594
29.48996925354004
13871.9970703125
6230.08984375
2798.095458984375
1256.708740234375
564.42529296875
1715.71630859375
770.573486328125
356.6585693359375
160.23675537109375
72.45661163330078
32.70658493041992
14.850712776184082
129.26226806640625
58.118675231933594
26.852537155151367
12.735808372497559
6.001351833343506
3.02862286567688
1.8745709657669067
1.1739387512207031
0.8658498525619507
0.7330876588821411
0.6773227453231812
0.6547468900680542
0.6459523439407349
0.6427088975906372
0.641668975353241
0.6414189338684082
0.641376256942749
0.6413705945014954
0.6413698196411133
0.6413693428039551
C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\sklearn\svm\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
0.6413694024085999
tensor([ 6.6692e-02, -5.3167e-03, -4.4601e-03,  ..., -6.7131e-03,
        -2.0658e-05, -8.7276e-03], requires_grad=True)
w=1.0, train_err=0.0, err=45.76499938964844
====================l1=========================
CMM train_err=0.0, err=7.704999923706055
8876.7744140625
4017.67724609375
24868.3203125
16329.0849609375
10161.625
6375.80224609375
19382.265625
11756.240234375
6610.4873046875
3446.762939453125
11769.66015625
4654.26953125
1488.9189453125
2621.6767578125
544.267822265625
625.0323486328125
3005.38671875
210133.5625
200409.828125
190964.796875
181813.25
172928.40625
164346.8125
156048.578125
148027.5625
140299.21875
132860.390625
125736.09375
118883.4296875
112293.1484375
105968.6875
99904.8828125
94091.890625
88538.4375
83226.7421875
78151.6484375
73339.28125
68774.203125
64428.3828125
60282.08203125
56345.21875
52615.4453125
49099.69921875
45779.41015625
42634.65234375
39668.7578125
36848.32421875
34196.84765625
31709.27734375
29355.1796875
27167.923828125
25769.5546875
23232.28125
21382.89453125
19655.939453125
18036.17578125
16513.798828125
15413.009765625
13827.2060546875
12862.431640625
17947.296875
13018.91796875
11899.8232421875
8957.76953125
8764.857421875
7276.5439453125
13746.1875
7422.17919921875
45518.671875
36722.81640625
29252.5859375
22991.6328125
17800.654296875
13589.5732421875
10324.81640625
7793.05517578125
7969.14501953125
9935.7451171875
6045.91455078125
5335.7177734375
11558.3271484375
6083.02001953125
4014.61865234375
11819.9599609375
17489.291015625
10225.845703125
5816.4248046875
3384.352783203125
92442.4609375
82955.9296875
74125.2734375
65914.640625
58355.29296875
51424.65625
45123.08203125
39367.328125
34154.59375
29448.00390625
25246.42578125
21497.568359375
18219.458984375
15397.783203125
12926.6044921875
10786.0888671875
8948.9287109375
7408.638671875
6682.8623046875
4979.26904296875
15337.5185546875
8422.4462890625
4636.91748046875
12592.4296875
7028.60986328125
4093.9609375
3017.600830078125
2289.200927734375
4435.43017578125
869.7655639648438
5169.59912109375
148091.078125
138333.984375
128981.28125
120039.546875
111514.5625
103393.828125
95684.1875
88376.875
81460.484375
74968.5390625
68855.2890625
63110.8046875
57710.8828125
52672.84375
47994.5234375
43622.6796875
39530.3515625
35729.4453125
32201.16015625
28982.119140625
26068.66015625
23400.056640625
20951.08203125
18725.96484375
16851.8984375
14801.736328125
13084.85546875
11532.2255859375
10109.826171875
8833.94921875
9196.4384765625
6768.0751953125
8209.013671875
5339.6533203125
4390.62353515625
13327.9345703125
7904.89501953125
5280.7509765625
3129.249267578125
4761944.0
4752022.0
4742104.0
4732216.0
4722351.0
4712487.0
4702627.0
4692771.0
4682941.0
4673112.0
4663314.0
4653527.0
Traceback (most recent call last):
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 427, in <module>
    margin_classifiers_perf(d=5000,n=200,approx_tau=1, SNR=10, n_test=1e4, s=1, l1=True)
  File "c:\Users\gizem\Documents\GitHub\noisy_fair\margin_no_imb.py", line 291, in margin_classifiers_perf
    optim.step()
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\optim\sgd.py", line 144, in step
    F.sgd(params_with_grad,
  File "C:\Users\gizem\anaconda3\envs\iw\lib\site-packages\torch\optim\_functional.py", line 186, in sgd
    buf.mul_(momentum).add_(d_p, alpha=1 - dampening)
KeyboardInterrupt